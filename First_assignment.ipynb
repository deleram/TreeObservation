{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing needed libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
      "0       2596      51      3                               258   \n",
      "1       2590      56      2                               212   \n",
      "2       2804     139      9                               268   \n",
      "3       2785     155     18                               242   \n",
      "4       2595      45      2                               153   \n",
      "\n",
      "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
      "0                               0                              510   \n",
      "1                              -6                              390   \n",
      "2                              65                             3180   \n",
      "3                             118                             3090   \n",
      "4                              -1                              391   \n",
      "\n",
      "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
      "0            221             232            148   \n",
      "1            220             235            151   \n",
      "2            234             238            135   \n",
      "3            238             238            122   \n",
      "4            220             234            150   \n",
      "\n",
      "   Horizontal_Distance_To_Fire_Points  ...  Soil_Type32  Soil_Type33  \\\n",
      "0                                6279  ...            0            0   \n",
      "1                                6225  ...            0            0   \n",
      "2                                6121  ...            0            0   \n",
      "3                                6211  ...            0            0   \n",
      "4                                6172  ...            0            0   \n",
      "\n",
      "   Soil_Type34  Soil_Type35  Soil_Type36  Soil_Type37  Soil_Type38  \\\n",
      "0            0            0            0            0            0   \n",
      "1            0            0            0            0            0   \n",
      "2            0            0            0            0            0   \n",
      "3            0            0            0            0            0   \n",
      "4            0            0            0            0            0   \n",
      "\n",
      "   Soil_Type39  Soil_Type40  Cover_Type  \n",
      "0            0            0           5  \n",
      "1            0            0           5  \n",
      "2            0            0           2  \n",
      "3            0            0           2  \n",
      "4            0            0           5  \n",
      "\n",
      "[5 rows x 55 columns]\n"
     ]
    }
   ],
   "source": [
    "# First step which is to load and dividing data to X and Y\n",
    "dfPath = r'data.csv'\n",
    "assignmentData = pd.read_csv(dfPath)\n",
    "print(assignmentData.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with null values: 0\n"
     ]
    }
   ],
   "source": [
    "# Check if any rows have null values\n",
    "null_rows = assignmentData.isnull().any(axis=1)\n",
    "print('Number of rows with null values:', sum(null_rows)) #  There were no null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows:  0\n"
     ]
    }
   ],
   "source": [
    "# Check if there is any duplicate rows\n",
    "duplicate_rows = assignmentData[assignmentData.duplicated()]\n",
    "print(\"Number of duplicate rows: \", len(duplicate_rows)) # none where found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
      "0       2596      51      3                               258   \n",
      "1       2590      56      2                               212   \n",
      "2       2804     139      9                               268   \n",
      "3       2785     155     18                               242   \n",
      "4       2595      45      2                               153   \n",
      "\n",
      "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
      "0                               0                              510   \n",
      "1                              -6                              390   \n",
      "2                              65                             3180   \n",
      "3                             118                             3090   \n",
      "4                              -1                              391   \n",
      "\n",
      "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
      "0            221             232            148   \n",
      "1            220             235            151   \n",
      "2            234             238            135   \n",
      "3            238             238            122   \n",
      "4            220             234            150   \n",
      "\n",
      "   Horizontal_Distance_To_Fire_Points  ...  Soil_Type31  Soil_Type32  \\\n",
      "0                                6279  ...            0            0   \n",
      "1                                6225  ...            0            0   \n",
      "2                                6121  ...            0            0   \n",
      "3                                6211  ...            0            0   \n",
      "4                                6172  ...            0            0   \n",
      "\n",
      "   Soil_Type33  Soil_Type34  Soil_Type35  Soil_Type36  Soil_Type37  \\\n",
      "0            0            0            0            0            0   \n",
      "1            0            0            0            0            0   \n",
      "2            0            0            0            0            0   \n",
      "3            0            0            0            0            0   \n",
      "4            0            0            0            0            0   \n",
      "\n",
      "   Soil_Type38  Soil_Type39  Soil_Type40  \n",
      "0            0            0            0  \n",
      "1            0            0            0  \n",
      "2            0            0            0  \n",
      "3            0            0            0  \n",
      "4            0            0            0  \n",
      "\n",
      "[5 rows x 54 columns]\n",
      "0    5\n",
      "1    5\n",
      "2    2\n",
      "3    2\n",
      "4    5\n",
      "Name: Cover_Type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = assignmentData.drop(columns=['Cover_Type'])\n",
    "y = assignmentData['Cover_Type']\n",
    "\n",
    "print(X.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second step : Divide data into train and test set. We chose test_size = 0.3 to get a 70-30 division.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Elevation    Aspect     Slope  Horizontal_Distance_To_Hydrology  \\\n",
      "183380   0.702466 -0.784220  0.386543                         -0.683720   \n",
      "561389  -1.431609  1.772207 -0.681631                         -1.069421   \n",
      "370958   0.003004 -0.962991 -0.414587                          3.126252   \n",
      "429888  -0.264648 -1.231147 -0.815152                         -1.125865   \n",
      "170657  -0.821363 -0.650141  1.187673                         -0.561425   \n",
      "\n",
      "        Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
      "183380                       -0.315721                         1.946292   \n",
      "561389                       -0.779104                        -0.152020   \n",
      "370958                        2.533231                         0.192671   \n",
      "429888                       -0.779104                        -0.348435   \n",
      "170657                       -0.092610                        -1.409466   \n",
      "\n",
      "        Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
      "183380       0.853218       -0.977752      -1.241733   \n",
      "561389      -0.265227        0.034056       0.376652   \n",
      "370958       0.443122       -0.421257      -0.510850   \n",
      "429888       0.107588        0.034056       0.089519   \n",
      "170657       1.188752       -1.483656      -1.946513   \n",
      "\n",
      "        Horizontal_Distance_To_Fire_Points  ...  Soil_Type31  Soil_Type32  \\\n",
      "183380                            0.308453  ...            0            0   \n",
      "561389                            0.170209  ...            0            0   \n",
      "370958                           -0.947828  ...            1            0   \n",
      "429888                           -0.009583  ...            0            0   \n",
      "170657                            0.376442  ...            0            0   \n",
      "\n",
      "        Soil_Type33  Soil_Type34  Soil_Type35  Soil_Type36  Soil_Type37  \\\n",
      "183380            0            0            0            0            0   \n",
      "561389            0            0            0            0            0   \n",
      "370958            0            0            0            0            0   \n",
      "429888            0            0            0            0            0   \n",
      "170657            0            0            0            0            0   \n",
      "\n",
      "        Soil_Type38  Soil_Type39  Soil_Type40  \n",
      "183380            0            0            0  \n",
      "561389            0            0            0  \n",
      "370958            0            0            0  \n",
      "429888            0            0            0  \n",
      "170657            0            0            0  \n",
      "\n",
      "[5 rows x 54 columns]\n"
     ]
    }
   ],
   "source": [
    "# Third step : It's time to preproccess\n",
    "# As we checked there is no missing values and no duplicate ones.\n",
    "# And all of our data is numerical, and there are no categorical columns that need to be converted to numerical values.\n",
    "# So we should just check if scaling is needed.\n",
    "\n",
    "# As we can see from the head our columns are not from the same range so we should scale the large ones\n",
    "\n",
    "cols_to_scale = ['Elevation', 'Aspect', 'Slope','Horizontal_Distance_To_Hydrology',# choose desired columns\n",
    "                 'Vertical_Distance_To_Hydrology','Horizontal_Distance_To_Roadways',\n",
    "                 'Hillshade_9am','Hillshade_Noon','Hillshade_3pm','Horizontal_Distance_To_Fire_Points']\n",
    "\n",
    "Scaler = StandardScaler() #define our scaler\n",
    "# scale our columns\n",
    "# It's noteworthy to say that we choose our scaling parameters(mean and sd) from our train data as \n",
    "# we don't have access to the test data. And we use those parameters to scale our test data too.\n",
    "Scaler.fit(X_train[cols_to_scale])\n",
    "X_train[cols_to_scale] = Scaler.transform(X_train[cols_to_scale])\n",
    "X_test[cols_to_scale] = Scaler.transform(X_test[cols_to_scale])\n",
    "\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'log_loss', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "{'criterion': 'entropy', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 150}\n"
     ]
    }
   ],
   "source": [
    "# You Can Uncomment all of these to see the GridSearch work but it takes a lot of time and i myself used a server to get the answers.\n",
    "# For saving time and making it less time_consuming for the reader i just set the best ones manually from the answers i got from the server.\n",
    "# In the documentation, I will share the screenshots and all the data.\n",
    "\n",
    "\n",
    "# Step 4: 5 fold validation\n",
    "five_fold = KFold(n_splits=5, shuffle=True, random_state=10)\n",
    "\n",
    "tree_parameters ={'max_depth': [20, 25],\n",
    "                    'criterion' : ['entropy', 'log_loss'],\n",
    "                    'min_samples_split': [5, 7],\n",
    "                    'min_samples_leaf' : [1, 2]}\n",
    "\n",
    "# Create a decision tree classifier model\n",
    "# tree_model = DecisionTreeClassifier()\n",
    "\n",
    "# # search for the best parameters\n",
    "# tree_grid = GridSearchCV(tree_model, tree_parameters, cv=five_fold, error_score=\"raise\")\n",
    "\n",
    "# # Fit the GridSearchCV objects to your data\n",
    "# tree_grid.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Best parameters for Decision Tree: \", tree_grid.best_params_)\n",
    "\n",
    "# # Printing all othe options:\n",
    "# results_df = pd.DataFrame(tree_grid.cv_results_)\n",
    "# results_df = results_df[['params', 'mean_test_score', 'std_test_score']]\n",
    "# results_df = results_df.sort_values(by=['mean_test_score'], ascending=False)\n",
    "# with pd.option_context('display.max_rows', None,\n",
    "#                        'display.max_columns', None,\n",
    "#                        'display.precision', 3,\n",
    "#                        'display.max_colwidth', None\n",
    "#                        ):\n",
    "#     print(results_df)\n",
    "\n",
    "Best_tree = {'criterion': 'log_loss', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
    "\n",
    "\n",
    "# Same thing for random forest\n",
    "forest_parameters =  {'n_estimators': [100, 150],\n",
    "                    'criterion' : ['entropy'],\n",
    "                    'min_samples_split': [5, 7],\n",
    "                    'min_samples_leaf' : [1, 2],\n",
    "                    }\n",
    "\n",
    "# # Create a random forest classifier model\n",
    "# forest_model = RandomForestClassifier()\n",
    "# forest_grid = GridSearchCV(forest_model, forest_parameters, cv=five_fold)\n",
    "# forest_grid.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters for each model\n",
    "\n",
    "# print(\"Best parameters for Random Forest: \", forest_grid.best_params_)\n",
    "\n",
    "# results_df = pd.DataFrame(forest_grid.cv_results_)\n",
    "# results_df = results_df[['params', 'mean_test_score', 'std_test_score']]\n",
    "# results_df = results_df.sort_values(by=['mean_test_score'], ascending=False)\n",
    "# with pd.option_context('display.max_rows', None,\n",
    "#                        'display.max_columns', None,\n",
    "#                        'display.precision', 3,\n",
    "#                        'display.max_colwidth', None\n",
    "#                        ):\n",
    "#     print(results_df)\n",
    "Best_forest = {'criterion': 'entropy', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 150}\n",
    "\n",
    "print(Best_tree)\n",
    "print(Best_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9391694969708096\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.94      0.94     63756\n",
      "           2       0.95      0.95      0.95     84864\n",
      "           3       0.93      0.94      0.93     10718\n",
      "           4       0.85      0.83      0.84       782\n",
      "           5       0.84      0.82      0.83      2875\n",
      "           6       0.89      0.88      0.88      5182\n",
      "           7       0.95      0.94      0.95      6127\n",
      "\n",
      "    accuracy                           0.94    174304\n",
      "   macro avg       0.91      0.90      0.90    174304\n",
      "weighted avg       0.94      0.94      0.94    174304\n",
      "\n",
      "[[59969  3482     3     0    51     7   244]\n",
      " [ 3730 80362   232     0   346   145    49]\n",
      " [    3   192 10049    79    28   367     0]\n",
      " [    0     0    95   652     0    35     0]\n",
      " [   52   423    30     0  2357    12     1]\n",
      " [    8   173   400    36     9  4556     0]\n",
      " [  335    34     1     0     1     0  5756]]\n"
     ]
    }
   ],
   "source": [
    "tree_model = DecisionTreeClassifier(criterion = \"log_loss\" , min_samples_leaf = 1, min_samples_split = 5)\n",
    "tree_model.fit(X_train,y_train)\n",
    "y_pred = tree_model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9533171929502479\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.97      0.94      0.95     63756\n",
      "           2       0.95      0.97      0.96     84864\n",
      "           3       0.94      0.96      0.95     10718\n",
      "           4       0.91      0.86      0.89       782\n",
      "           5       0.94      0.77      0.85      2875\n",
      "           6       0.93      0.89      0.91      5182\n",
      "           7       0.97      0.94      0.96      6127\n",
      "\n",
      "    accuracy                           0.95    174304\n",
      "   macro avg       0.94      0.91      0.92    174304\n",
      "weighted avg       0.95      0.95      0.95    174304\n",
      "\n",
      "[[59901  3704     1     0    14     4   132]\n",
      " [ 1774 82664   182     1   112   103    28]\n",
      " [    2   152 10294    40    16   214     0]\n",
      " [    0     0    90   674     0    18     0]\n",
      " [   37   554    41     0  2227    16     0]\n",
      " [    6   163   362    23     3  4625     0]\n",
      " [  320    24     0     0     1     0  5782]]\n"
     ]
    }
   ],
   "source": [
    "forest_model = RandomForestClassifier(criterion = \"entropy\" , min_samples_leaf = 1, min_samples_split = 5, n_estimators= 150 )\n",
    "forest_model.fit(X_train,y_train)\n",
    "y_pred = forest_model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
